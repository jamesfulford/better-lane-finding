{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Lane Finding Project\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "- [x] Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "- [x] Apply a distortion correction to raw images.\n",
    "- [x] Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "- [x] Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "- [x] Detect lane pixels and fit to find the lane boundary.\n",
    "- [x] Determine the curvature of the lane and vehicle position with respect to center.\n",
    "- [x] Warp the detected lane boundaries back onto the original image.\n",
    "- [x] Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# builtins\n",
    "from glob import glob\n",
    "import functools\n",
    "import os\n",
    "\n",
    "# third-party\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# project-specific\n",
    "from src.calibrate import calibrate\n",
    "from src.thresholding import region_of_interest, thresh, and_binary, or_binary, not_binary\n",
    "from src.image_mappings import direction, magnitude, abs_sobel\n",
    "from src.perspective import get_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finding all calibration images...\")\n",
    "calibration_image_paths = sorted(glob('./camera_cal/calibration*.jpg'), key=lambda p: (len(p), p))\n",
    "calibration_images = list(map(cv2.imread, calibration_image_paths))\n",
    "for path, image in zip(calibration_image_paths, calibration_images):\n",
    "    print(\"{0} ({1[0]}w, {1[1]}h)\".format(\n",
    "        os.path.basename(path),\n",
    "        image.shape\n",
    "    ))\n",
    "print(\"Found {} images.\".format(len(calibration_images)))\n",
    "\n",
    "print(\"Calibrating...\")\n",
    "used_calibration_images, object_points, all_corners, undistort, mtx, dist = calibrate(\n",
    "    calibration_images, \n",
    "    chessboard_dimensions=(9, 6), \n",
    "    to_grayscale_flag=cv2.COLOR_BGR2GRAY\n",
    ")\n",
    "print(\"Calibrated using {} chessboard images (was unable to use {} images).\".format(\n",
    "    len(used_calibration_images), len(calibration_images) - len(used_calibration_images)\n",
    "))\n",
    "\n",
    "print(\"Writing to output_images/chessboards...\")\n",
    "directory = \"output_images/chessboards\"\n",
    "try:\n",
    "    os.mkdir(directory)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for image, corners, i in zip(used_calibration_images, all_corners, range(len(used_calibration_images))):\n",
    "    img = cv2.drawChessboardCorners(image, (9, 6), corners, True)\n",
    "    cv2.imwrite(directory + \"/{}.before.jpg\".format(i), cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "    out = undistort(img)\n",
    "    cv2.imwrite(directory + \"/{}.after.jpg\".format(i), cv2.cvtColor(out, cv2.COLOR_RGB2BGR))\n",
    "    if i == 0:\n",
    "        plt.imshow(out)\n",
    "\n",
    "print(\"Written. Sample output with corners drawn:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perspective Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Defining perspective transformers...\")\n",
    "to_birds_eye, to_car_perspective = get_transformers(np.float32([\n",
    "    [600, 450],\n",
    "    [683, 450],\n",
    "    [1060, 690],\n",
    "    [253, 690],\n",
    "]), np.float32([\n",
    "    [350, 0],\n",
    "    [950, 0],\n",
    "    [950, 720],\n",
    "    [350, 720],\n",
    "]))\n",
    "\n",
    "img = undistort(cv2.cvtColor(cv2.imread(\"test_images/test1.jpg\"), cv2.COLOR_BGR2RGB))\n",
    "\n",
    "f, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(25, 9))\n",
    "ax1.imshow(img)\n",
    "ax2.imshow(to_birds_eye(img))\n",
    "ax3.imshow(to_car_perspective(to_birds_eye(img)))\n",
    "print(\"Done. Showing sample image, birds_eye, then conversion back:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Defining image selectors...\")\n",
    "\n",
    "# The number fiddling below was gleaned from courses.\n",
    "# I tried fiddling with numbers on my own. It brought endless frustration. I almost gave up on the entire program.\n",
    "# The only way I could get passed this emotional roadblock was to go on to the next project and see what I would miss if I quit.\n",
    "# Glad I didn't quit here.\n",
    "# -James Fulford\n",
    "\n",
    "def mag_thresh(gray):\n",
    "    \"\"\"Assumes grayscale image\"\"\"\n",
    "    return thresh(magnitude(\n",
    "        gray,\n",
    "        sobel_kernel=9\n",
    "    ), (30, 100))\n",
    "\n",
    "def dir_thresh(gray):\n",
    "    \"\"\"Assumes grayscale image\"\"\"\n",
    "    return thresh(direction(\n",
    "        gray,\n",
    "        sobel_kernel=15\n",
    "    ), (0.7, 1.3))\n",
    "\n",
    "def gray_thresh(gray):\n",
    "    \"\"\"Assumes grayscale image\"\"\"\n",
    "    return thresh(direction(\n",
    "        gray,\n",
    "    ), (180, 256))\n",
    "\n",
    "def r_thresh(img):\n",
    "    \"\"\"Assumes RGB\"\"\"\n",
    "    return thresh(\n",
    "        img[:,:,0],\n",
    "        (200, 255)\n",
    "    )\n",
    "\n",
    "def s_thresh(hls):\n",
    "    \"\"\"Assumes HLS\"\"\"\n",
    "    return thresh(\n",
    "        hls[:,:,2],\n",
    "        (170, 255)  # (90, 255)\n",
    "    )\n",
    "\n",
    "def sobelx_thresh(gray):\n",
    "    \"\"\"Assumes Grayscale\"\"\"\n",
    "    return thresh(abs_sobel(\n",
    "        gray,\n",
    "        orient=\"x\",\n",
    "        sobel_kernel=3,\n",
    "    ), (20, 100))\n",
    "\n",
    "def h_thresh(hls):\n",
    "    \"\"\"Assumes HLS\"\"\"\n",
    "    return thresh(\n",
    "        hls[:,:,0],\n",
    "        (15, 100)\n",
    "    )\n",
    "\n",
    "print(\"Defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Showing samples using the selectors...\")\n",
    "\n",
    "img = to_birds_eye(undistort(cv2.cvtColor(cv2.imread(\"test_images/test1.jpg\"), cv2.COLOR_BGR2RGB)))\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "\n",
    "f, (\n",
    "    (ax11, ax12, ax13, ax14),\n",
    "    (ax21, ax22, ax23, ax24),\n",
    "    (ax31, ax32, ax33, ax34),\n",
    ") = plt.subplots(nrows=3, ncols=4, figsize=(24, 9))\n",
    "f.tight_layout()\n",
    "\n",
    "# Original image\n",
    "ax11.imshow(img)\n",
    "\n",
    "# Building blocks\n",
    "ax12.imshow(mag_thresh(gray), cmap=\"gray\")\n",
    "ax13.imshow(dir_thresh(gray), cmap=\"gray\")\n",
    "ax14.imshow(gray_thresh(gray), cmap=\"gray\")\n",
    "\n",
    "ax21.imshow(r_thresh(img), cmap=\"gray\")\n",
    "ax22.imshow(s_thresh(hls), cmap=\"gray\")\n",
    "ax23.imshow(sobelx_thresh(gray), cmap=\"gray\")\n",
    "ax24.imshow(h_thresh(hls), cmap=\"gray\")\n",
    "\n",
    "# Useful-looking compositions\n",
    "ax31.imshow(or_binary(sobelx_thresh(gray), s_thresh(hls)), cmap=\"gray\")  # comp1\n",
    "ax32.imshow(or_binary(r_thresh(img), s_thresh(hls)), cmap=\"gray\")  # comp2\n",
    "ax33.imshow(and_binary(not_binary(h_thresh(hls)), dir_thresh(gray)), cmap=\"gray\")  # comp3\n",
    "\n",
    "# Pixels which show up in at least 2 of the above compositions (inefficient calculation)\n",
    "ax34.imshow(or_binary(\n",
    "    or_binary(\n",
    "        and_binary(or_binary(sobelx_thresh(gray), s_thresh(hls)), or_binary(r_thresh(img), s_thresh(hls))),\n",
    "        and_binary(or_binary(sobelx_thresh(gray), s_thresh(hls)), and_binary(not_binary(h_thresh(hls)), dir_thresh(gray))),\n",
    "    ),\n",
    "    and_binary(or_binary(r_thresh(img), s_thresh(hls)), and_binary(not_binary(h_thresh(hls)), dir_thresh(gray))),\n",
    "), cmap=\"gray\")\n",
    "\n",
    "plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building final threshold filter...\")\n",
    "\n",
    "def comp1(img, gray, hls):\n",
    "    \"\"\"\n",
    "    First composition. Resembles composition given in course. Takes out too much data.\n",
    "    \"\"\"\n",
    "    return or_binary(sobelx_thresh(gray), s_thresh(hls))\n",
    "\n",
    "def comp2(img, gray, hls):\n",
    "    \"\"\"\n",
    "    Second composition. Can sometimes be noisy. Captures color well (right saturation or redness).\n",
    "    \"\"\"\n",
    "    return or_binary(r_thresh(img), s_thresh(hls))\n",
    "\n",
    "def comp3(img, gray, hls):\n",
    "    \"\"\"\n",
    "    Third composition. Performs very well on concrete. \n",
    "    h_thresh gets concrete exactly wrong, so its inverse is spot-on.\n",
    "    On asphault, not_binary(h_thresy) can be noisy, so and'd with semi-random noisy dir_thresh to temper the noise.\n",
    "    \"\"\"\n",
    "    return and_binary(not_binary(h_thresh(hls)), dir_thresh(gray))\n",
    "\n",
    "def lane_selector(img):\n",
    "    \"\"\"\n",
    "    Keeps pixels which appear in at least 2 of my composed images.\n",
    "    Assumes RGB\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "    \n",
    "    c1 = comp1(img, gray, hls)\n",
    "    c2 = comp2(img, gray, hls)\n",
    "    c3 = comp3(img, gray, hls)\n",
    "    return or_binary(\n",
    "        and_binary(c1, c2),\n",
    "        or_binary(\n",
    "            and_binary(c1, c3),\n",
    "            and_binary(c2, c3),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "plt.imshow(lane_selector(img), cmap=\"gray\")  # using image loaded from earlier cell\n",
    "print(\"Done. Sample image:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Lane Pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Defining lane pixel detectors...\")\n",
    "\n",
    "ym_per_pix = 3.0 / 100  # meters per pixel in y dimension (dashed lines are 3 meters long per US regulations)\n",
    "xm_per_pix = 3.7 / 620  # meters per pixel in x dimension (lane is 3.7 meters wide per US regulations)\n",
    "\n",
    "def get_lane_start(img):\n",
    "    histogram = np.sum(img[img.shape[0]//2:,:], axis=0)\n",
    "    midpoint = np.int(histogram.shape[0]//2)\n",
    "    leftx_base = np.argmax(histogram[:midpoint])\n",
    "    rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "    return (leftx_base, rightx_base)\n",
    "\n",
    "def get_lane_pixels(img, nwindows=9, margin=100, minpix=50):\n",
    "    window_height = np.int(img.shape[0] // nwindows)\n",
    "    nonzero = img.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    # to debug:\n",
    "    # out_img = np.dstack((img, img, img))\n",
    "\n",
    "    lane_inds = [\n",
    "        [],  # left lane\n",
    "        [],  # right lane\n",
    "    ]\n",
    "    current = list(get_lane_start(img))  # leftx_current, rightx_current\n",
    "\n",
    "    for window in range(nwindows):\n",
    "        window_boundaries = list(\n",
    "            map(lambda b: (b - margin, b + margin), current)\n",
    "        )\n",
    "        win_y_low = img.shape[0] - (window + 1) * window_height\n",
    "        win_y_high = img.shape[0] - window * window_height\n",
    "        \n",
    "        for i, bound in enumerate(window_boundaries):\n",
    "            leftx, rightx = bound\n",
    "            \n",
    "            # to debug:\n",
    "            # cv2.rectangle(out_img, (leftx, win_y_low), (rightx, win_y_high), (0, 255, 0), 2) \n",
    "\n",
    "            good_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "            (nonzerox >= leftx) &  (nonzerox < rightx)).nonzero()[0]\n",
    "            \n",
    "            lane_inds[i].append(good_inds)\n",
    "            \n",
    "            if len(good_inds) > minpix:\n",
    "                current[i] = np.int(np.mean(nonzerox[good_inds]))\n",
    "\n",
    "    # to debug:\n",
    "    # plt.imshow(out_img)\n",
    "    lane_inds = list(map(np.concatenate, lane_inds))\n",
    "    return [(nonzerox[inds], nonzeroy[inds]) for inds in lane_inds ]\n",
    "\n",
    "\n",
    "def fit_polynomials(img, ym_per_pix=ym_per_pix, xm_per_pix=xm_per_pix):\n",
    "    \"\"\"\n",
    "    Returns tuple of lists, equal in length:\n",
    "    - First is coefficients of each quadratic polynomial\n",
    "    - Second is a series of x values to graph for each polynomial\n",
    "    \"\"\"\n",
    "    ploty = np.linspace(0, img.shape[0] - 1, img.shape[0])    \n",
    "    lane_pixels = get_lane_pixels(img)\n",
    "    quads = list(map(\n",
    "        lambda vp: np.polyfit(vp[1] * ym_per_pix, vp[0] * xm_per_pix, 2),\n",
    "        lane_pixels,\n",
    "    ))\n",
    "    return quads, list(map(\n",
    "        lambda fit: fit[0] * (ploty ** 2)\n",
    "            + fit[1] * (ploty ** 1)\n",
    "            + fit[2], \n",
    "        quads,\n",
    "    ))\n",
    "\n",
    "def curvature(y_eval, quad, ym_per_pix=ym_per_pix):\n",
    "    (a, b, c) = quad\n",
    "    return ((1 + (2 * a * (y_eval * ym_per_pix) + b) ** 2) ** 1.5) / np.absolute(2 * a)\n",
    "\n",
    "def get_lane_offset_from_center(image, pixel_x_values, xm_per_pix=xm_per_pix):\n",
    "    # Find lane offsets in pixel space, convert to real\n",
    "    lane_bottom_px_xvalues = list(map(lambda f: f[-1], [left_fit, right_fit]))\n",
    "    lane_center_px_x = sum(lane_bottom_px_xvalues) / len(lane_bottom_px_xvalues)\n",
    "    camera_location_px_x = image.shape[1] / 2\n",
    "    return (camera_location_px_x - lane_center_px_x) * xm_per_pix\n",
    "\n",
    "img = lane_selector(to_birds_eye(undistort(cv2.cvtColor(cv2.imread(\"test_images/test1.jpg\"), cv2.COLOR_BGR2RGB))))\n",
    "# Make quadratics in pixel space\n",
    "quads, [ left_fit, right_fit ] = fit_polynomials(img, ym_per_pix=1, xm_per_pix=1)\n",
    "# Make quadratics in real space\n",
    "real_quads, [ real_left_fit, real_right_fit ] = fit_polynomials(img)\n",
    "\n",
    "# Find curvatures in real space\n",
    "ploty = np.linspace(0, img.shape[0] - 1, img.shape[0])\n",
    "y_eval = np.max(ploty)\n",
    "print(\"Curvatures (m): \", list(map(\n",
    "    lambda q: curvature(y_eval, q),\n",
    "    real_quads,\n",
    ")))\n",
    "\n",
    "print(\"Offset from lane center (m): \", get_lane_offset_from_center(image, [ left_fit, right_fit ]))\n",
    "\n",
    "plt.plot(left_fit, ploty, color='red')\n",
    "plt.plot(right_fit, ploty, color='red')\n",
    "plt.imshow(img)\n",
    "\n",
    "print(\"Done. Showing fit on sample image:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Defining full pipeline...\")\n",
    "\n",
    "def full_pipeline(img):\n",
    "    \"\"\"\n",
    "    Given distorted camera image in RGB,\n",
    "    returns undistorted camera image (based on calibration) with lane zone and annotations added.\n",
    "    Holds no state between successive calls.\n",
    "    \"\"\"\n",
    "    undist = undistort(img)\n",
    "    warped = to_birds_eye(undist)\n",
    "    binary_warped = lane_selector(warped)\n",
    "    quads, [ left_fit, right_fit ] = fit_polynomials(binary_warped, ym_per_pix=1, xm_per_pix=1)\n",
    "    real_quads, [ real_left_fit, real_right_fit ] = fit_polynomials(binary_warped)  # real space\n",
    "    \n",
    "    # Create an image to draw the lines on\n",
    "    warp_zero = np.zeros_like(binary_warped).astype(np.uint8)\n",
    "    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "    \n",
    "    # Drawing a polygon for the zone\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fit, ploty]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fit, ploty])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "    cv2.fillPoly(color_warp, np.int_([pts]), (0, 255, 0))  # draw zone as green\n",
    "    \n",
    "    # Warping zone image to car's perspective\n",
    "    newwarp = to_car_perspective(color_warp)\n",
    "    \n",
    "    # Image with zone drawn\n",
    "    result = cv2.addWeighted(undist, 1, newwarp, 0.3, 0)\n",
    "    \n",
    "    # TODO(jamesfulford): Add annotations\n",
    "    return result\n",
    "\n",
    "print(\"Drawing zone on sample road...\")\n",
    "img = cv2.cvtColor(cv2.imread(\"test_images/test5.jpg\"), cv2.COLOR_BGR2RGB)\n",
    "result = full_pipeline(img)\n",
    "plt.imshow(result)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Producing pipeline middle step sample images...\")\n",
    "\n",
    "images = sorted(glob(\"test_images/*.jpg\"), key=lambda p: (len(p), p))\n",
    "\n",
    "for image_name in images:\n",
    "    img = cv2.imread(image_name)\n",
    "    directory = \"output_images/\" + os.path.basename(image_name)\n",
    "    try:\n",
    "        os.mkdir(directory)\n",
    "    except:\n",
    "        pass\n",
    "    print(directory)\n",
    "\n",
    "    cv2.imwrite(directory + \"/00_original.jpg\", img)\n",
    "    \n",
    "    # Undistort image (virtue of camera lens)\n",
    "    undis = undistort(img)\n",
    "    cv2.imwrite(directory + \"/01_undistorted.jpg\", undis)\n",
    "    \n",
    "    # Perspective transform\n",
    "    birds_eye = to_birds_eye(undis)\n",
    "    cv2.imwrite(directory + \"/02_perspective_transform.jpg\", birds_eye)\n",
    "\n",
    "    # Lanes\n",
    "    lanes = lane_selector(birds_eye)\n",
    "    plt.imsave(directory + \"/03_lane_selector.jpg\", lanes, cmap='gray')\n",
    "    \n",
    "    # Fitting polynomials\n",
    "    ploty = np.linspace(0, lanes.shape[0] - 1, lanes.shape[0])\n",
    "    quads, [ left_fit, right_fit ] = fit_polynomials(lanes, ym_per_pix=1, xm_per_pix=1)\n",
    "    \n",
    "    plt.imshow(lanes)\n",
    "    plt.plot(left_fit, ploty, color='red')\n",
    "    plt.plot(right_fit, ploty, color='red')\n",
    "    plt.savefig(directory + \"/04_plotted_lines.jpg\")\n",
    "    plt.clf()\n",
    "plt.close()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_name in images:\n",
    "    img = cv2.imread(image_name)\n",
    "    directory = \"output_images/\" + os.path.basename(image_name)\n",
    "    try:\n",
    "        os.mkdir(directory)\n",
    "    except:\n",
    "        pass\n",
    "    print(directory)\n",
    "    \n",
    "    plt.imsave(directory + \"/05_pipeline_result.jpg\", full_pipeline(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
